{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# This script generates data simulating history of shipments to a warehouse. \n",
    "# The simulation is based on the article: \n",
    "#   Michael Lingzhi L, Elliott Wolf, Daniel Wintz (2020): Duration-of-stay storage assignment\n",
    "#   under uncertainty. Published as a conference paper at ICLR 2020. http://arXiv:1903.05063v3 [cs.LG] 1Feb 2020\n",
    "# The script is organized as follows:\n",
    "# 1. _prodt_ table is formed containing categorical information about the shipment - \n",
    "#    supplier, customer, outbound location, customer type, product group, product packaging, \n",
    "#    nutritional value, content, expiration period, reference to the trend and seasonality function (TSF) of demand\n",
    "# 2. In while cycle from start date to end date of the history, fact data of deliveries are calculated. The fact data are date, \n",
    "#    volume (number of pallets), total storage time/duration, number of pallets unsold until the expiration date, \n",
    "#    threshold value of the duration, percentiles of cumulative duration of storage (DoS) distribution for pallets. \n",
    "#    Each iteration of the loop goes through _prodt_ and the result is added to the dfData set.\n",
    "# 3. The fields supplier, product group, packaging, nutritional value, product content are combined \n",
    "#    into a text description. 3 different orders are used to combine field values. The order of the fields \n",
    "#    is chosen randomly. The resulting descriptions are stored in field dfData.txt. \n",
    "# 4. Reference tables for customers, outbound locations, suppliers, product groups are formed and saved\n",
    "# 5. dfData is saved in the inidata.csv file\n",
    "\n",
    "WORKING_DIRECTORY = 'C:/Pilot/test/'\n",
    "os.chdir (WORKING_DIRECTORY)\n",
    "\n",
    "fpLog = Path ('log.txt')\n",
    "\n",
    "with open (fpLog, 'w') as flog:\n",
    "    print ('Start data simulation at : ', datetime.now(), file = flog)\n",
    "\n",
    "# intersection of supplier and nutritional value\n",
    "# 'NA' means none\n",
    "supnt = pd.DataFrame(\n",
    "    {\n",
    "        'SUP01' : ['FATFR','LCTFR','SGRFR','DIET','NA',np.nan,np.nan],\n",
    "        'SUP02' : ['ORG','PRVFR',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SUP03' : ['FATFR','LCTFR','SGRFR','DIET','PRVFR','NA',np.nan],\n",
    "        'SUP04' : ['FATFR','LCTFR','SGRFR','DIET','PRVFR','ORG','NA'],\n",
    "        'SUP05 CHEES' : ['ORG','NA',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SUP06 ICRM' : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of supplier and product group\n",
    "suppr = pd.DataFrame(\n",
    "    {\n",
    "        'SUP01' : ['MLK','CURD','ICRM','KFR','FBM','CHEES','CHEES SLC','SCRM','JGRT','PIZZA'],\n",
    "        'SUP02' : ['MLK','CURD','ICRM','KFR','FBM','CHEES','CHEES SLC','SCRM','JGRT','PIZZA'],\n",
    "        'SUP03' : ['MLK','CURD','KFR','FBM','SCRM',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SUP04' : ['MLK','CURD','JGRT','ICRM',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SUP05 CHEES' : ['CHEES','CHEES SLC','PIZZA',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SUP06 ICRM' : ['ICRM',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of product group and volume\n",
    "prvol = pd.DataFrame(\n",
    "    {\n",
    "        'MLK': ['100ml','300ml','500ml','1L','2L','5L'],\n",
    "        'CURD': ['100g','300g','500g','1kg','NA',np.nan],\n",
    "        'ICRM': ['100g','300g','500g','1kg',np.nan,np.nan],\n",
    "        'KFR': ['100ml','300ml','500ml','1L',np.nan,np.nan],\n",
    "        'FBM': ['100ml','300ml','500ml','1L',np.nan,np.nan],\n",
    "        'CHEES': ['100g','300g','500g','1kg','HEAD',np.nan],\n",
    "        'CHEES SLC': ['100g',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'SCRM': ['100ml','300ml',np.nan,np.nan,np.nan,np.nan],\n",
    "        'JGRT': ['100ml','300ml','500ml','1L','2L',np.nan],\n",
    "        'PIZZA': ['25cm','30cm',np.nan,np.nan,np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of product group and packaging\n",
    "prpak = pd.DataFrame(\n",
    "    {\n",
    "        'MLK': ['PBTL','GBTL','PCNR','TPAK'],\n",
    "        'CURD': ['PGLS','PCNT',np.nan,np.nan],\n",
    "        'ICRM': ['PRCNT',np.nan,np.nan,np.nan],\n",
    "        'KFR': ['PBTL','GBTL','TPAK',np.nan],\n",
    "        'FBM': ['PBTL','GBTL','TPAK',np.nan],\n",
    "        'CHEES': ['PCNT','VACUM',np.nan,np.nan],\n",
    "        'CHEES SLC': ['PCNT',np.nan,np.nan,np.nan],\n",
    "        'SCRM': ['PGLS',np.nan,np.nan,np.nan],\n",
    "        'JGRT': ['PCNT',np.nan,np.nan,np.nan],\n",
    "        'PIZZA': ['VACUM','PRCNT',np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of volume and packaging\n",
    "vlpak = pd.DataFrame(\n",
    "    {\n",
    "        '100ml': ['PGLS','PCNT',np.nan,np.nan,np.nan],\n",
    "        '300ml': ['PGLS','PCNT','PBTL',np.nan,np.nan],\n",
    "        '500ml': ['PGLS','PCNT','PBTL','GBTL',np.nan],\n",
    "        '1L': ['PCNT','PBTL','GBTL','PCNR','TPAK'],\n",
    "        '2L': ['PBTL','PCNR',np.nan,np.nan,np.nan],\n",
    "        '5L': ['PCNR',np.nan,np.nan,np.nan,np.nan],\n",
    "        '100g': ['PCNT','VACUM','PRCNT',np.nan,np.nan],\n",
    "        '300g': ['PCNT','VACUM','PRCNT',np.nan,np.nan],\n",
    "        '500g': ['PCNT','VACUM','PRCNT',np.nan,np.nan],\n",
    "        '1kg': ['PCNT','VACUM','PRCNT',np.nan,np.nan],\n",
    "        'HEAD': ['PCNT','VACUM','PRCNT',np.nan,np.nan],\n",
    "        '25cm': ['PRCNT','VACUM',np.nan,np.nan,np.nan],\n",
    "        '30cm': ['PRCNT','VACUM',np.nan,np.nan,np.nan],    \n",
    "    }\n",
    ")\n",
    "# intersection of product group and content\n",
    "prcnt = pd.DataFrame(\n",
    "    {\n",
    "        'MLK': ['1.0%','1.5%','3.2%','3.6%','5%'],\n",
    "        'CURD': ['1.0%','5%','9%',np.nan,np.nan],\n",
    "        'ICRM': ['CREM','FI','NA',np.nan,np.nan],\n",
    "        'KFR': ['0.5%','1.5%','2.5%','5%',np.nan],\n",
    "        'FBM': ['3.2%','4%','6%',np.nan,np.nan],\n",
    "        'CHEES': ['25%','45%','50%','60%',np.nan],\n",
    "        'CHEES SLC': ['25%','45%','50%','60%',np.nan],\n",
    "        'SCRM': ['10%','15%','20%','25%','30%'],\n",
    "        'JGRT': ['1.0%','5%','9%',np.nan,np.nan],\n",
    "        'PIZZA': ['TCK','TIN',np.nan,np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of product group and nutrition value\n",
    "prntr = pd.DataFrame(\n",
    "    {\n",
    "        'MLK': ['FATFR','LCTFR','NA',np.nan,np.nan],\n",
    "        'CURD': ['FATFR','NA',np.nan,np.nan,np.nan],\n",
    "        'ICRM': ['LCTFR','DIET','NA',np.nan,np.nan],\n",
    "        'KFR': ['DIET','NA',np.nan,np.nan,np.nan],\n",
    "        'FBM': ['DIET','NA',np.nan,np.nan,np.nan],\n",
    "        'CHEES': ['ORG','LWFAT','NA',np.nan,np.nan],\n",
    "        'CHEES SLC': ['ORG','LWFAT','NA',np.nan,np.nan],\n",
    "        'SCRM': ['ORG','NA',np.nan,np.nan,np.nan],\n",
    "        'JGRT': ['PRVFR','NA',np.nan,np.nan,np.nan],\n",
    "        'PIZZA': ['VEGN','DIET','NA',np.nan,np.nan]\n",
    "    }\n",
    ")\n",
    "# intersection of content and nutrition value\n",
    "cnval = pd.DataFrame(\n",
    "    {\n",
    "        '0.5%': ['FATFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '1.0%': ['FATFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '1.5%': ['FATFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '2.5%': ['DIET','ORG','PRVFR','LCTFR','NA',np.nan],\n",
    "        '3.2%': ['DIET','ORG','PRVFR','LCTFR','NA',np.nan], \n",
    "        '3.6%': ['DIET','ORG','PRVFR','LCTFR','NA',np.nan],\n",
    "        '5%'  : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan], \n",
    "        '6%'  : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        '9%'  : ['ORG','PRVFR','LCTFR','NA',np.nan,np.nan],\n",
    "        'CREM': ['LCTFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        'FI'  : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        '10%' : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        '15%' : ['NA',np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        '20%' : ['PRVFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '30%' : ['ORG','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '25%' : ['ORG','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '45%' : ['PRVFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '50%' : ['PRVFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        '60%' : ['ORG','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        'TCK' : ['ORG','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        'TIN' : ['PRVFR','NA',np.nan,np.nan,np.nan,np.nan],\n",
    "        'NA'  : ['DIET','ORG','PRVFR','LCTFR','NA',np.nan]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Form dataframe _prodt_ which joins supplier, product group, volume, packing, content, nutrient value \n",
    "# Note: for brevity, in all scripts 'product' means 'product group'\n",
    "\n",
    "suppr = suppr.melt (var_name ='supplier', value_name ='product_')\n",
    "suppr = (suppr := suppr[suppr.product_.notna()]).rename (columns ={'product_': 'product'})\n",
    "supnt = (dt := supnt.melt (var_name ='supplier', value_name ='nvalue'))[dt.nvalue.notna()]\n",
    "\n",
    "suplr = suppr.merge (supnt, left_on ='supplier', right_on ='supplier')\n",
    "\n",
    "vlpak = (dt := vlpak.melt (var_name ='volume',  value_name ='packing'))[dt.packing.notna()]\n",
    "prpak = (dt := prpak.melt (var_name ='product', value_name ='packing'))[dt.packing.notna()]\n",
    "prvol = (dt := prvol.melt (var_name ='product', value_name ='volume' ))[dt.volume.notna() ]\n",
    "prcnt = (dt := prcnt.melt (var_name ='product', value_name ='content'))[dt.content.notna()]\n",
    "prntr = (dt := prntr.melt (var_name ='product', value_name ='nvalue' ))[dt.nvalue.notna() ]\n",
    "cnval = (dt := cnval.melt (var_name ='content', value_name ='nvalue' ))[dt.nvalue.notna() ]\n",
    "\n",
    "prodt = prvol.merge (vlpak, on ='volume')\n",
    "prodt = prodt.merge (prpak, how ='inner', on =('packing','product'))\n",
    "\n",
    "prodt_ = prcnt.merge (cnval, on ='content')\n",
    "prodt_ = prodt_.merge(prntr, how = 'inner', on =('nvalue','product'))\n",
    "\n",
    "_prodt_ = prodt.merge(prodt_, on ='product')\n",
    "_prodt_ = suplr.merge(_prodt_, how = 'inner', on =('nvalue','product'))\n",
    "\n",
    "# Add product's expiration period ('term'). Expiration period is determined by product group, \n",
    "# product group and packing, product group and content, product group and nvalue. \n",
    "# Expiration period is one of the most important features for forecasting. Therefore 'term' is not included in\n",
    "# input data. This info can only be obtained from descriptions. Expiration period is used \n",
    "# to calculate sales charts only. \n",
    "\n",
    "arex =[\n",
    "            {'product': 'CURD',   'packing': np.nan, 'content': '1.0%', 'nvalue': np.nan, 'term': 25}, \n",
    "            {'product': 'CURD',   'packing': np.nan, 'content': '5%',   'nvalue': np.nan, 'term': 25}, \n",
    "            {'product': 'CURD',   'packing': np.nan, 'content': '9%',   'nvalue': np.nan, 'term': 15}, \n",
    "            {'product': 'CURD',   'packing': 'NA',   'content': np.nan, 'nvalue': np.nan, 'term': 5 }, \n",
    "            {'product': 'JGRT',   'packing': np.nan, 'content': '1.0%', 'nvalue': np.nan, 'term': 25}, \n",
    "            {'product': 'JGRT',   'packing': np.nan, 'content': '5%',   'nvalue': np.nan, 'term': 25}, \n",
    "            {'product': 'JGRT',   'packing': np.nan, 'content': '9%',   'nvalue': np.nan, 'term': 15}, \n",
    "            {'product': 'JGRT',   'packing': np.nan, 'content': np.nan, 'nvalue':'PRVFR', 'term': 5 }, \n",
    "            {'product': 'MLK',    'packing': 'GBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 5 },\n",
    "            {'product': 'MLK',    'packing': 'PBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 20}, \n",
    "            {'product': 'MLK',    'packing': 'PCNR', 'content': np.nan, 'nvalue': np.nan, 'term': 20}, \n",
    "            {'product': 'MLK',    'packing': 'TPAK', 'content': np.nan, 'nvalue': np.nan, 'term': 60}, \n",
    "            {'product': 'SCRM',   'packing': np.nan, 'content': '10%',  'nvalue': np.nan, 'term': 15}, \n",
    "            {'product': 'SCRM',   'packing': np.nan, 'content': '15%',  'nvalue': np.nan, 'term': 15}, \n",
    "            {'product': 'SCRM',   'packing': np.nan, 'content': '20%',  'nvalue': np.nan, 'term': 10}, \n",
    "            {'product': 'SCRM',   'packing': np.nan, 'content': '25%',  'nvalue': np.nan, 'term': 10}, \n",
    "            {'product': 'SCRM',   'packing': np.nan, 'content': '30%',  'nvalue': np.nan, 'term': 10}, \n",
    "            {'product': 'ICRM',   'packing': np.nan, 'content': np.nan, 'nvalue': np.nan, 'term': 60}, \n",
    "            {'product': 'KFR',    'packing': 'GBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 5 },\n",
    "            {'product': 'KFR',    'packing': 'PBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 20}, \n",
    "            {'product': 'KFR',    'packing': 'TPAK', 'content': np.nan, 'nvalue': np.nan, 'term': 60}, \n",
    "            {'product': 'FBM',    'packing': 'GBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 5 },\n",
    "            {'product': 'FBM',    'packing': 'PBTL', 'content': np.nan, 'nvalue': np.nan, 'term': 20}, \n",
    "            {'product': 'FBM',    'packing': 'TPAK', 'content': np.nan, 'nvalue': np.nan, 'term': 60}, \n",
    "            {'product': 'CHEES',   'packing': np.nan, 'content': np.nan, 'nvalue': np.nan, 'term': 30}, \n",
    "            {'product': 'CHEES SLC','packing': np.nan, 'content': np.nan, 'nvalue': np.nan, 'term': 20}, \n",
    "            {'product': 'PIZZA',  'packing': np.nan, 'content': np.nan, 'nvalue': np.nan, 'term': 120} \n",
    "        ]\n",
    "ex = pd.DataFrame (arex)\n",
    "exnul = ex[['packing','content','nvalue']].isnull() \n",
    "exnul['allnan'] = (exnul['packing'] & exnul['content'] & exnul['nvalue'])\n",
    "exnul['cvnnan'] = ((~exnul['packing']) & exnul['content'] & exnul['nvalue'])\n",
    "exnul['pvnnan'] = (exnul['packing'] & (~exnul['content']) & exnul['nvalue'])\n",
    "exnul['pkcnan'] = (exnul['packing'] & exnul['content'] & (~exnul['nvalue']))\n",
    "_prodt_ = _prodt_.merge(ex[exnul['allnan']][['product','term']], how = 'left', on=('product'))\n",
    "_prodt_ = _prodt_.rename(columns={'term': \"term0\"})\n",
    "_prodt_ = _prodt_.merge(ex[exnul['cvnnan']][['product','packing','term']], how = 'left', on=('product','packing'))\n",
    "_prodt_ = _prodt_.rename(columns={'term': \"term1\"})\n",
    "_prodt_ = _prodt_.merge(ex[exnul['pvnnan']][['product','content','term']], how = 'left', on=('product','content'))\n",
    "_prodt_ = _prodt_.rename(columns={'term': \"term2\"})\n",
    "_prodt_ = _prodt_.merge(ex[exnul['pkcnan']][['product','nvalue', 'term']], how = 'left', on=('product','nvalue' ))\n",
    "_prodt_ = _prodt_.rename(columns={'term': \"term3\"})\n",
    "_prodt_['term'] = _prodt_[['term0','term1','term2','term3']].min (axis = 1)\n",
    "_prodt_ = _prodt_[['supplier', 'product', 'nvalue', 'volume', 'packing', 'content', 'term']]\n",
    "\n",
    "_prodt_.sort_values (\n",
    "    by = ['supplier', 'product', 'nvalue', 'volume', 'packing', 'content', 'term'], \n",
    "    ignore_index = True\n",
    "    ).to_html (Path ('ref_descriptions.html'))\n",
    "\n",
    "# Form reference table of descriptions. I use 'did' as input feature for forecasting models \n",
    "# as if we received and structured all the information from the descriptions without errors.\n",
    "# 'Did' is not included in input data for product identification. \n",
    "\n",
    "dfdesc = _prodt_.loc (axis = 1)[['supplier', 'product', 'nvalue', 'volume', 'packing', 'content']]\n",
    "dfdesc['did'] = np.arange (1, dfdesc.shape[0] + 1, dtype = int)\n",
    "\n",
    "# Create reference table prlt which assigns TSFs to product groups. \n",
    "\n",
    "prlt = pd.DataFrame(\n",
    "    {\n",
    "        'product': ['MLK','CURD','ICRM','KFR','FBM','CHEES','CHEES SLC','SCRM','JGRT','PIZZA'],\n",
    "#        'lt'     : [    0,     1,    4,    2,    2,    3,        3,     1,     0,     3]\n",
    "        'lt'     : [    0,     1,    1,    1,    0,    0,         1,     1,     0,     1]\n",
    "    }\n",
    ")\n",
    "\n",
    "_prodt_ = _prodt_.merge (prlt, how = 'left', on=('product'))\n",
    "\n",
    "HLENGTH  = 6    # duration of the history is given by number of years \n",
    "INFINITY = 13   # auxiliary arrays are calculated for a duration greater than HLENGTH with a margin \n",
    "                # INFINITY is given by the number of years as well\n",
    "\n",
    "dstart = date.fromisoformat ('2019-01-01')                      # warehouse start date\n",
    "dstop  = dstart + relativedelta (years = HLENGTH, days = -1)    # end date of history\n",
    "d64start = np.datetime64('2019-01-01', 'D')                     # warehouse start date in datetime64 format/type\n",
    "ihdays = (dstop - dstart).days + 1                              # duration of the history in days\n",
    "\n",
    "# Define TSFs\n",
    "\n",
    "# In the code commented bellow function fnTrend2 calculates TSF with pooly conditioned year seasonality.\n",
    "# TSF lags half a period every odd year. \n",
    "# Then 2 variants of such TSF are formed with period of 4 and 2 months\n",
    "\n",
    "def fnTrend2 (ipmonths, imaxpoint, k1, b):\n",
    "\n",
    "# ipmonths is period of TSF and has to contain integer number of months. \n",
    "# For example, 3 means that demand repeats every 3 months \n",
    "# imaxpoint contains the coordinate of TSF's maximum point for even periods. \n",
    "# imaxpoint is specified in days from the beginning of the period.\n",
    "# k determines a slope of TSF, b determines a value at the minimum point\n",
    "\n",
    "    lY = []\n",
    "\n",
    "    for i in range (INFINITY*12//ipmonths):\n",
    "        dbm = dstart + relativedelta (months = i*ipmonths)\n",
    "        dem = dstart + relativedelta (months = (i+1)*ipmonths, days = -1)\n",
    "        ipdays = (dem - dbm).days + 1\n",
    "        nax = np.arange (ipdays)\n",
    "        nay = np.arange (ipdays, dtype = np.float32)\n",
    "        sls1  = slice (0, imaxpoint + 1)\n",
    "        sls2x = slice (0, ipdays - imaxpoint - 1)\n",
    "        sls2y = slice (imaxpoint + 1, ipdays)\n",
    "        nay[sls1] = nax[sls1]*k1\n",
    "        k2 = nay[imaxpoint]/(ipdays - imaxpoint - 1)\n",
    "        nay[sls2y] = nay[imaxpoint] - (nax[sls2x] + 1)*k2\n",
    "        lY = lY + (nay + b).tolist()\n",
    "    \n",
    "    naY1 = np.asarray (lY)\n",
    "    naY2 = np.roll (naY1, ipdays//2)\n",
    "    naX = np.arange (dstart, dstart + relativedelta (days = naY1.shape[0]), timedelta (days = 1), dtype = date) \n",
    "    naY = np.asarray([naY1[i] if naX[i].year%2 == 0 else naY2[i] for i in range(naY1.shape[0])])\n",
    "\n",
    "    return naY#, naY1, naY2, naX\n",
    "'''\n",
    "MAXPOINT = 60  #number of days\n",
    "PERIOD   = 4   #number of months\n",
    "k1 = 0.6\n",
    "b  = 5\n",
    "\n",
    "btrend00 = fnTrend2 (PERIOD, MAXPOINT, k1, b)\n",
    "#naY, naY1, naY2, naX = fnTrend (PERIOD, MAXPOINT, k1, b)\n",
    "\n",
    "#pd.DataFrame (naY[0:2199], columns = ['TSF1']).to_csv (Path ('picture01.csv'), index = False)\n",
    "#pd.DataFrame (naY1[0:2199], columns = ['TSF2']).to_csv (Path ('picture02.csv'), index = False)\n",
    "\n",
    "MAXPOINT = 30  #number of days\n",
    "PERIOD   = 2   #number of months\n",
    "k1 = 1.2\n",
    "b  = 5.0\n",
    "\n",
    "btrend01 = fnTrend2 (PERIOD, MAXPOINT, k1, b)\n",
    "'''\n",
    "\n",
    "# fnTrend1 calculates TSF with well conditioned year seasonality.\n",
    "# 2 variants of such TSF are formed with period 4 and 2 months\n",
    "\n",
    "def fnTrend1 (ipmonths, imaxpoint, k1, b):\n",
    "\n",
    "# ipmonths is period of TSF and has to contain integer number of months. \n",
    "# For example, 3 means that demand repeats every 3 months \n",
    "# imaxpoint contains the coordinate of TSF's maximum point. \n",
    "# imaxpoint is specified in days from the beginning of the period.\n",
    "# k determines a slope of TSF, b determines a value at the minimum point\n",
    "\n",
    "    lY = []\n",
    "\n",
    "    for i in range (INFINITY*12//ipmonths):\n",
    "        dbm = dstart + relativedelta (months = i*ipmonths)\n",
    "        dem = dstart + relativedelta (months = (i+1)*ipmonths, days = -1)\n",
    "        ipdays = (dem - dbm).days + 1\n",
    "        nax = np.arange (ipdays)\n",
    "        nay = np.arange (ipdays, dtype = np.float32)\n",
    "        sls1  = slice (0, imaxpoint + 1)\n",
    "        sls2x = slice (0, ipdays - imaxpoint - 1)\n",
    "        sls2y = slice (imaxpoint + 1, ipdays)\n",
    "        nay[sls1] = nax[sls1]*k1\n",
    "        k2 = nay[imaxpoint]/(ipdays - imaxpoint - 1)\n",
    "        nay[sls2y] = nay[imaxpoint] - (nax[sls2x] + 1)*k2\n",
    "        lY = lY + (nay + b).tolist()\n",
    "\n",
    "    return np.asarray (lY)\n",
    "\n",
    "MAXPOINT = 60  #number of days\n",
    "PERIOD   = 4   #number of months\n",
    "k1 = 0.6\n",
    "b  = 5\n",
    "\n",
    "btrend00 = fnTrend1 (PERIOD, MAXPOINT, k1, b)\n",
    "\n",
    "MAXPOINT = 30  #number of days\n",
    "PERIOD   = 2   #number of months\n",
    "k1 = 1.2\n",
    "b  = 5.0\n",
    "\n",
    "btrend01 = fnTrend1 (PERIOD, MAXPOINT, k1, b)\n",
    "\n",
    "# fnWave calculates smooth TSFs which are not used in the current version of \n",
    "# this script (see reference table prlt) \n",
    "\n",
    "def fnWave (ipmonths, ampli, b):\n",
    "\n",
    "    lY = []\n",
    "\n",
    "    for i in range (INFINITY*12//ipmonths):\n",
    "        dbm = dstart + relativedelta (months = i*ipmonths)\n",
    "        dem = dstart + relativedelta (months = (i+1)*ipmonths, days = -1)\n",
    "        ipdays = (dem - dbm).days + 1\n",
    "        nax = np.arange (ipdays)\n",
    "        nay = np.arange (ipdays, dtype = np.float32)\n",
    "        nay = (np.sin (2*3.14*nax/ipdays) + b)*ampli\n",
    "        lY = lY + nay.tolist()\n",
    "\n",
    "    return np.asarray (lY)\n",
    "\n",
    "# Trend #2: Wave per month\n",
    "\n",
    "PERIOD = 1      #number of months \n",
    "AMPLIFIER = 10\n",
    "b = 1.5\n",
    "\n",
    "btrend02 = fnWave (PERIOD, AMPLIFIER, b)\n",
    "\n",
    "# Trend #3: Wave per quater\n",
    "\n",
    "PERIOD = 3      #number of months \n",
    "AMPLIFIER = 20\n",
    "b = 1.5\n",
    "\n",
    "btrend03 = fnWave (PERIOD, AMPLIFIER, b)\n",
    "\n",
    "# Trend #4: Wave per year\n",
    "\n",
    "PERIOD = 12      #number of months \n",
    "AMPLIFIER = 40\n",
    "b = 1.5\n",
    "\n",
    "btrend04 = fnWave (PERIOD, AMPLIFIER, b)\n",
    "\n",
    "trends = np.ndarray (shape = (5, btrend00.shape[0]))\n",
    "trends[0][:] = btrend00\n",
    "trends[1][:] = btrend01\n",
    "trends[2][:] = btrend02\n",
    "trends[3][:] = btrend03\n",
    "trends[4][:] = btrend04\n",
    "\n",
    "# Add clients/customers ('client'), client/customer types ('tid'), outbound locations (ol). \n",
    "# Inbound location (il) is equal to supplier. \n",
    "# Add the following reference tables: \n",
    "# clnt lists clients and their types\n",
    "# ctpr bundles clients and product groups\n",
    "# olil bundles outbound locations and suppliers\n",
    "# clol bundles clients and outbound locations\n",
    "# outl specifies a gain for each outbound location\n",
    "\n",
    "# Client type has 3 values: restaurants - 1, hotels - 2, shops - 3. \n",
    "# Table ctpr (client type x product group) determines some restrictions:  \n",
    "# - restaurants consume cheese, ice cream and milk,  \n",
    "# - hotels pizza, ice cream, milk, cheese,  \n",
    "# - stores take away all product groups. \n",
    "\n",
    "clnt = pd.DataFrame(\n",
    "    {\n",
    "        'client': ['CLN01','CLN02','CLN03','CLN04','CLN05'],\n",
    "        'tid'   : [      1,      1,      2,      3,      3]\n",
    "    }\n",
    ")\n",
    "ctpr = pd.DataFrame(\n",
    "    {\n",
    "        'CLN01' : ['MLK','ICRM','CHEES',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'CLN02' : ['MLK','ICRM','CHEES',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'CLN03' : ['MLK','ICRM','CHEES','PIZZA',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "        'CLN04' : ['MLK','CURD','ICRM','KFR','FBM','CHEES','CHEES SLC','SCRM','JGRT','PIZZA'],\n",
    "        'CLN05' : ['MLK','CURD','ICRM','KFR','FBM','CHEES','CHEES SLC','SCRM','JGRT','PIZZA'],\n",
    "    }\n",
    ")\n",
    "clol = pd.DataFrame(\n",
    "    {\n",
    "        'CLN01' : ['LO1',np.nan],\n",
    "        'CLN02' : ['LO2',np.nan],\n",
    "        'CLN03' : ['LO3',np.nan],\n",
    "        'CLN04' : ['LO4', 'LO5'],\n",
    "        'CLN05' : ['LO6', 'LO7'],\n",
    "    }\n",
    ")\n",
    "olil = pd.DataFrame(\n",
    "    {\n",
    "        'LO1' : ['SUP01',       np.nan,      np.nan],\n",
    "        'LO2' : ['SUP05 CHEES','SUP04',      np.nan],\n",
    "        'LO3' : ['SUP05 CHEES','SUP03','SUP06 ICRM'],\n",
    "        'LO4' : ['SUP01',       np.nan,      np.nan],\n",
    "        'LO5' : ['SUP01',       np.nan,      np.nan],\n",
    "        'LO6' : ['SUP02',       np.nan,      np.nan],\n",
    "        'LO7' : ['SUP02',       np.nan,      np.nan]\n",
    "    }\n",
    ")\n",
    "outl = pd.DataFrame(\n",
    "    {\n",
    "        'oloc': ['LO1','LO2','LO3','LO4','LO5','LO6','LO7'],\n",
    "        'ampli':[  1.4,  1.6,  2.1,  4.8,  5.1,  5.4,  4.2]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add client and outbound location to dataframe _prodt_   \n",
    "\n",
    "ctpr = ctpr.melt (var_name='client', value_name='product_')\n",
    "ctpr = ctpr[ctpr.product_.notna()].rename (columns={'product_': 'product'})\n",
    "\n",
    "clol = clol.melt (var_name='client', value_name='oloc')\n",
    "clol = clol[clol.oloc.notna()]\n",
    "\n",
    "ctpr = ctpr.merge (clol, on='client')\n",
    "\n",
    "olil = olil.melt (var_name='oloc', value_name='supplier')\n",
    "olil = olil[olil.supplier.notna()]\n",
    "\n",
    "olpr = olil.merge (suppr, how = 'inner', on='supplier')\n",
    "\n",
    "olpr = olpr.merge (ctpr, how = 'inner', on=('product','oloc'))\n",
    "olpr = olpr.merge (outl, how = 'inner', on='oloc')\n",
    "\n",
    "_prodt_ = _prodt_.merge (olpr, how = 'inner', on =('supplier', 'product'))\n",
    "\n",
    "# Prepare while-cycle to add shipments\n",
    "# Sales charts are calculated for shipments in the array sales.  \n",
    "# The diff array is needed to calculate DoS. On the cumulative sales chart, \n",
    "# I build a spline of the first order and calculate percentiles from the spline.  \n",
    "# However, if there were no sales, then the same chart value corresponds to different days.  \n",
    "# This results in an error when calculating percentiles. To get around the ambiguity, \n",
    "# minor additions to the cumulative graph are calculated in diff for the absence of sales.\n",
    "# The array k contains coefficients that determine decline in the demand \n",
    "# curve for products of the calculated shipments.\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "START_PERIOD = 30 # dates of first shipments are taken from interval 0 - START_PERIOD\n",
    "MAXTERM = 120     # defines a maximum size of sales chart\n",
    "\n",
    "sales0   = np.zeros ((_prodt_.shape[0], MAXTERM + 1))\n",
    "save     = np.zeros ((_prodt_.shape[0], MAXTERM))\n",
    "na_diff_ = np.zeros (sales0.shape)\n",
    "k        = np.zeros (_prodt_.shape[0])\n",
    "\n",
    "# _prodt_.nsl stores unsold part (overdue product) of shipment\n",
    "# _prodt_.drn contains total sale time of shipment.\n",
    "# _prodt_.qnt is volume of shipment.\n",
    "# _prodt_.stop is threshold time/duration of shipment. If the total sale time of shipment \n",
    "# twice less than the threshold time volume of next shipment increses twice   \n",
    "\n",
    "_prodt_['nsl'] = 0.0\n",
    "_prodt_['drn'] = np.int32(0)\n",
    "_prodt_['qnt'] = 0.0\n",
    "_prodt_['stop']= np.int32(0)\n",
    "\n",
    "lhash = ['supplier', 'product', 'nvalue', 'volume', 'packing', 'content', 'term', 'lt', 'client', 'oloc']\n",
    "_prodt_['phash'] = _prodt_.loc(axis = 1)[lhash].apply (lambda x: hash(tuple(x)), axis = 1)\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('TERM x LT = ', _prodt_.loc(axis=1)[['term','lt']].drop_duplicates(), file = flog)\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('_prodt_.columns = ', _prodt_.columns, file = flog)\n",
    "    print ('_prodt_.shape = ', _prodt_.shape, file = flog)\n",
    "    lcatdesc = ['supplier', 'product', 'nvalue', 'volume', 'packing', 'content']\n",
    "    print ('SUPPLIER x PRODUCT x NVALUE x VOLUME x PACKING x CONTENT = ', _prodt_.loc(axis=1)[lcatdesc].drop_duplicates().shape[0], file = flog)\n",
    "\n",
    "IND = 432\n",
    "HASHID = _prodt_.phash.loc(axis = 0)[IND]\n",
    "lIND = _prodt_.index[_prodt_.phash == HASHID].tolist()\n",
    "RNUMBER = _prodt_.index.get_loc (lIND[0]) if len(lIND) > 0 else -100\n",
    "\n",
    "count = 0   # while-cycle counter\n",
    "\n",
    "# Shipments' Calculation Cycle\n",
    "# A new shipment is calculated for each row of _prodt_. \n",
    "# If a shipment date (see below) + the total sale time of the shipment is \n",
    "# greater than the end date of the history, then the line is removed from _prodt_\n",
    "# The loop ends when the number of rows in prodt is 0.\n",
    "\n",
    "while _prodt_.shape[0] > 0:\n",
    "\n",
    "    with open(fpLog, 'a') as flog:\n",
    "        print ('COUNT = ', count, file = flog)\n",
    "\n",
    "    # Since the number of rows in _prodt_ can change, at the beginning of each iteration \n",
    "    # main arrays are resized to match the number of rows in _prodt_.\n",
    "\n",
    "    sales0   = np.resize (sales0, (_prodt_.shape[0], MAXTERM + 1))\n",
    "    save     = np.resize (sales0, (_prodt_.shape[0], MAXTERM))\n",
    "    na_diff_ = np.resize (na_diff_, sales0.shape)\n",
    "    k        = np.resize (k,       _prodt_.shape[0])\n",
    "    sales0.fill(0), save.fill(0), na_diff_.fill(0), k.fill(0)\n",
    "\n",
    "    # _prodt_.start is the shipment date. It is the number of days from the start of the warehouse. \n",
    "    # If the cycle counter is equal to zero, then the shipment date is randomly selected from \n",
    "    # the interval 0 - START_PERIOD. If the cycle counter is greater than zero, then \n",
    "    # the shipment date is calculated as shipment date of the previous shipment + the interval for \n",
    "    # which the previous shipment was sold (prodt.drn) + a random number of days from 1 to 4.\n",
    "\n",
    "    if count == 0:\n",
    "        _prodt_['start'] = rng.integers (low = 0, high = START_PERIOD, size = _prodt_.shape[0], dtype = np.int32, endpoint = True)\n",
    "    else:\n",
    "        nadelays = rng.integers (low = 1, high = 4, size = _prodt_.shape[0], dtype = np.int32, endpoint = True)\n",
    "        _prodt_['start'] = _prodt_['start'] + _prodt_['drn'] + nadelays\n",
    "    \n",
    "    sales = sales0[:, 1:]\n",
    "    sales += np.arange (MAXTERM)\n",
    "    \n",
    "    # Initialize auxiliary arrays for expiration periods and demand gains\n",
    "\n",
    "    term   = _prodt_['term'].to_numpy()\n",
    "    ampli  = _prodt_['ampli'].to_numpy()\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('term:', file = flog)\n",
    "            print (term[RNUMBER], file = flog)\n",
    "            print ('nsl:',  file = flog)\n",
    "            print (_prodt_.nsl[RNUMBER], file = flog)\n",
    "            print ('qnt:',  file = flog)\n",
    "            print (_prodt_.qnt[RNUMBER], file = flog)\n",
    "\n",
    "    # If at the previous iteration the supply was not completely sold, then the volume \n",
    "    # of the current supply is halved\n",
    "    \n",
    "    srcond = (_prodt_.nsl > 0.0)\n",
    "    _prodt_.loc[srcond, 'qnt'] = _prodt_.loc[srcond, 'qnt']/2.0 \n",
    "\n",
    "    # If the previous shipment was sold two times faster than the threshold time, \n",
    "    # then volume of current shipment is doubled. New threshold time (see below) will be equal to \n",
    "    # duration (the total sale time) of current shipment. \n",
    "\n",
    "    srcond = (_prodt_.stop/_prodt_.drn >= 2.0)\n",
    "    _prodt_.loc[srcond, 'qnt'] = _prodt_.loc[srcond, 'qnt']*2.0\n",
    "    _prodt_.loc[srcond, 'stop'] = 0\n",
    "    _prodt_.loc(axis = 1)['qnt'] = np.floor (_prodt_.loc(axis = 1)['qnt']) \n",
    "\n",
    "    # Calculate slop of demand curves\n",
    "    # Broadcasting and vectorization are used to minimise time of calculation\n",
    "\n",
    "    k[...] = np.where (term > 10, 1.45/(0.2*term), 0.55/(0.5*term))\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('k:', file = flog)\n",
    "            print (k[RNUMBER], file = flog)\n",
    "\n",
    "    # Calculate TSFs and adding noise \n",
    "\n",
    "    sales[...] = (np.maximum (-np.tanh (k*(sales.T - term)), 0)*rng.normal(1, 0.25, size = (MAXTERM, _prodt_.shape[0]))).T\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nFirst sales:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "\n",
    "    # Form intervals of TDFs which correspond to intervals of shipments sales\n",
    "        \n",
    "    lts = [trends[lt][istart : istart + MAXTERM] for istart, lt in zip(_prodt_['start'], _prodt_['lt'])]\n",
    "    \n",
    "    alts = np.asarray (lts)\n",
    "    \n",
    "    # Calculate sale charts \n",
    "\n",
    "    sales[...] = ((sales*alts).T*ampli).T\n",
    "    sales[...] = sales.clip (min = 0)\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales after alts:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "\n",
    "    if count == 0:\n",
    "    # Calculate shipment volumes of the first shipments\n",
    "        _prodt_.qnt = np.sum (sales, axis = 1)*rng.uniform (0.5, 0.8, size = _prodt_.shape[0])\n",
    "        if RNUMBER > 0:\n",
    "            with open (fpLog, 'a') as flog:\n",
    "                print ('_prodt_.qnt:', file = flog)\n",
    "                print (_prodt_.qnt[RNUMBER], file = flog)\n",
    "\n",
    "    # Cut sales charts by threshold volume (_prodt_.qnt)\n",
    "    # Note: since cumulative chart values may be not equal \n",
    "    # to threshold volume it is needed later to set the last value of chart cutted \n",
    "    # equal to _prodt_.qnt overwise values of _prodt_.qnt will decrease every itaration\n",
    "\n",
    "    sales[...] = np.cumsum (sales, axis = 1)\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales after cumsum:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "\n",
    "    save[...] = sales.copy()\n",
    "\n",
    "    sales *= (sales.T <= _prodt_.qnt.to_numpy()).T\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales after _prodt_.qnt:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "\n",
    "    sales[...] = np.where (sales.max (axis = 1) == 0.0, 0.0001, sales.T).T\n",
    "\n",
    "    # Get duration of sale as time when cumulative chart achieve maximum value\n",
    "\n",
    "    sales_max = sales.max (axis = 1)\n",
    "\n",
    "    _prodt_.drn = np.array ([(np.isclose(x, sales_max[ind])).nonzero()[0].min() + 1 for ind, x in enumerate (list (sales))])\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('_prodt_.drn: ', _prodt_.drn[RNUMBER], file = flog)\n",
    "\n",
    "    sales[...] = np.where (sales_max < 1, sales.T, np.floor (sales.T)).T\n",
    "    sales_max = sales.max (axis = 1)\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales after floor:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "            print ('\\nSales_max:', file = flog)\n",
    "            print (sales_max[RNUMBER], file = flog)\n",
    "\n",
    "    if count == 0:\n",
    "    #   Assign adjusted values to _prodt_.qnt after rounding sales charts\n",
    "        _prodt_.loc(axis = 1)['qnt']  = sales_max\n",
    "    #   The first value of threshold time is duration of the first shipment\n",
    "        _prodt_.loc(axis = 1)['stop'] = _prodt_.drn\n",
    "\n",
    "    if RNUMBER > 0:\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('_prodt_.qnt:', file = flog)\n",
    "            print (_prodt_.qnt[RNUMBER], file = flog)\n",
    "\n",
    "    # If duration of shipment sale is less then expiration period then \n",
    "    # the last value of cumulative sales chart is getting equal to _prodt_.qnt\n",
    "    # This assignmeht allows to avoid gradual decrease of shipment volume.\n",
    "\n",
    "    srcond = (_prodt_.drn < term)\n",
    "    if RNUMBER > 0 and RNUMBER in _prodt_[srcond].index :\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nWE ARE BEFORE BASE SECTION', file = flog)\n",
    "            print ('srcond:', file = flog)\n",
    "            print (srcond, file = flog)    \n",
    "    ixmask = np.ix_(srcond.to_numpy(), )\n",
    "    if RNUMBER > 0 and RNUMBER in _prodt_[srcond].index :\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('ixmask:', file = flog)\n",
    "            print (ixmask, file = flog)\n",
    "    if ixmask[0].size > 0 and count != 0:\n",
    "        smask = sales[ixmask]\n",
    "        for ind, x in enumerate (list (_prodt_.drn.to_numpy()[ixmask])):\n",
    "            smask[ind, x - 1] = _prodt_.loc[srcond, 'qnt'].to_numpy()[ind]\n",
    "        sales[ixmask] = smask\n",
    "        sales_max[ixmask] = smask.max (axis = 1)\n",
    "        _prodt_.loc[srcond, 'nsl'] = 0\n",
    "        _prodt_.loc[_prodt_.stop == 0, 'stop'] = _prodt_.drn\n",
    "        if RNUMBER > 0 and RNUMBER in _prodt_[srcond].index :\n",
    "            with open (fpLog, 'a') as flog:\n",
    "                print ('\\nWE ARE IN BASE SECTION', file = flog)\n",
    "                print ('_prodt_.qnt:', file = flog)\n",
    "                print (_prodt_.qnt[RNUMBER], file = flog)\n",
    "\n",
    "    if RNUMBER > 0 :\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales after qnt correction:', file = flog)\n",
    "            print (sales[RNUMBER], file = flog)\n",
    "\n",
    "    # If shipment is not saled completly than threshold time is setting to expiration period \n",
    "    # _prodt_.nsl gets value of not saled volume and \n",
    "    # shipment volume becomes equals to volume soled\n",
    "\n",
    "    srcond = (_prodt_.drn == term)\n",
    "    ixmask = np.ix_(srcond.to_numpy(), )\n",
    "    _prodt_.loc[srcond, 'stop'] = _prodt_[srcond].drn\n",
    "    _prodt_.loc[srcond, 'nsl'] = (_prodt_[srcond].qnt - sales_max[ixmask]).clip (lower = 0)\n",
    "    _prodt_.loc[srcond, 'qnt'] = sales_max[ixmask]\n",
    "    \n",
    "    if RNUMBER > 0 :\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nAfter nsl update:', file = flog)\n",
    "            print ('_prodt_.stop: ', _prodt_.stop[RNUMBER], file = flog)\n",
    "            print ('_prodt_.nsl:  ', _prodt_.nsl[RNUMBER],  file = flog)\n",
    "            print ('_prodt_.qnt:  ', _prodt_.qnt[RNUMBER],  file = flog)\n",
    "\n",
    "    # Percentile calculation.  \n",
    "    # For ease of calculation, I use splines. To get percentiles, you need to have \n",
    "    # a function inverse to CDF. If there are periods on the cumulative sales chart where \n",
    "    # there are no sales, the inverse function is ambiguous.  \n",
    "    # To get around this limitation, I add monotonically increasing values ​​<< 1 \n",
    "    # to the sales graph at the points of no sales.\n",
    "\n",
    "    sales[...] = (sales.T/sales.max(axis = 1)).T\n",
    " \n",
    "    na_diff = na_diff_[:,1:]\n",
    "    na_diff[np.isclose(np.diff (sales0, n = 1, axis = -1), 0.0).nonzero()] = 1\n",
    "    na_diff[...] = np.cumsum (na_diff, -1)*na_diff/10000.0\n",
    "\n",
    "    _spln_ = []\n",
    "    \n",
    "    if RNUMBER > 0 :\n",
    "        with open (fpLog, 'a') as flog:\n",
    "            print ('\\nSales0 before splines:', file = flog)\n",
    "            print (sales0[RNUMBER], file = flog)\n",
    "            print ('\\nna_diff before splines:', file = flog)\n",
    "            print (na_diff[RNUMBER], file = flog)\n",
    "            print ('\\nnadrn before splines:', file = flog)\n",
    "            print (_prodt_.drn[RNUMBER], file = flog)\n",
    "\n",
    "    try:\n",
    "        for ind, x in enumerate (list (sales0)):\n",
    "            _slice_ = slice (0, _prodt_.drn[ind] + 1)\n",
    "            x[_slice_] += na_diff_[ind, _slice_]\n",
    "            fCDF = sp.interpolate.splrep (x[_slice_], np.arange(_prodt_.drn[ind] + 1), k=1)\n",
    "            ptil = np.arange (0, 1.0, 0.049999999)\n",
    "            sCDF = sp.interpolate.splev (ptil, fCDF)\n",
    "            _spln_.append (sCDF)\n",
    "\n",
    "    except Exception as e:\n",
    "        print ('ind = ' + str(ind))\n",
    "        print ('_prodt_.drn[ind] = ' + str(_prodt_.drn[ind]))\n",
    "        print (x[slice(0, _prodt_.drn[ind])])\n",
    "        print ('count = ', count)\n",
    "        raise e\n",
    "\n",
    "    dfCDF = pd.DataFrame (_spln_, columns = ['T00', 'T01','T02','T03','T04','T05','T06','T07','T08','T09','T10','T11','T12','T13','T14','T15','T16','T17','T18','T19','T20'])\n",
    "\n",
    "    if count == 0:\n",
    "        dfData = pd.merge (_prodt_, dfCDF, left_index=True, right_index=True)\n",
    "    else:\n",
    "        dfEpoch = pd.merge (_prodt_, dfCDF, left_index=True, right_index=True)\n",
    "        dfData = pd.concat ([dfData, dfEpoch], axis = 0, ignore_index = True)\n",
    "\n",
    "    _prodt_.drop (_prodt_[(_prodt_.start + _prodt_['drn']) > ihdays].index, inplace = True)\n",
    "    _prodt_.reset_index (drop = True, inplace = True)\n",
    "\n",
    "    lIND = _prodt_.index[_prodt_.phash == HASHID].tolist()\n",
    "    RNUMBER = _prodt_.index.get_loc (lIND[0]) if len(lIND) > 0 else -100\n",
    "\n",
    "    count += 1\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('\\ndfData.shape befoe drop: ', dfData.shape, file = flog) \n",
    "\n",
    "dfData.drop (dfData[dfData['qnt'] < 1].index, inplace = True)\n",
    "dfData.reset_index (drop = True, inplace = True)\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('\\ndfData.shape after drop: ', dfData.shape, file = flog) \n",
    "\n",
    "# Create reference table dwf for words in descriptions\n",
    "\n",
    "lwords = dfData['supplier'].unique().tolist() \n",
    "lwords.extend (dfData['product'].unique().tolist())\n",
    "lwords.extend (dfData['nvalue' ].unique().tolist())\n",
    "lwords.extend (dfData['volume' ].unique().tolist())\n",
    "lwords.extend (dfData['packing'].unique().tolist())\n",
    "lwords.extend (dfData['content'].unique().tolist())\n",
    "\n",
    "lwords = [y for x in lwords for y in x.split()]\n",
    "dfw = pd.DataFrame (list (set (lwords)), columns =['word'])\n",
    "id  = np.arange (1, dfw.shape[0] + 1, dtype=int)\n",
    "rng.shuffle (id)\n",
    "dfw['wid'] = id\n",
    "filepath = Path('ref_words.html')  \n",
    "dfw.sort_values(['wid']).to_html (filepath, index = False)\n",
    "\n",
    "# To distort the descriptions 3 variants of the word order are introduced and \n",
    "# reassigned randomly to shipments\n",
    "\n",
    "dfData.insert (0, 'txtv', rng.integers (low = 1, high = 4, size = dfData.shape[0]))\n",
    "\n",
    "conditions = [(dfData['txtv'] == 1), (dfData['txtv'] == 2), (dfData['txtv'] == 3)]\n",
    "\n",
    "values = [\n",
    "    dfData['supplier'] + ' ' + dfData['product'] + ' ' +\n",
    "    dfData['nvalue']   + ' ' + dfData['volume']  + ' ' + \n",
    "    dfData['packing']  + ' ' + dfData['content'],\n",
    "    dfData['supplier'] + ' ' + dfData['packing'] + ' ' +\n",
    "    dfData['volume']   + ' ' + dfData['product'] + ' ' + \n",
    "    dfData['nvalue']   + ' ' + dfData['content'],\n",
    "    dfData['product']  + ' ' + dfData['supplier']+ ' ' +\n",
    "    dfData['volume']   + ' ' + dfData['nvalue']  + ' ' + \n",
    "    dfData['content']  + ' ' + dfData['packing']\n",
    "    ]\n",
    "\n",
    "dfData.insert (0, 'txt', np.select (conditions, values))\n",
    "dfData['txt'] = dfData['txt'].replace ({'NA': ''}, regex = True)\n",
    "\n",
    "# Split text resulted into words and encode each word using the referense table dfw\n",
    "#dfText = dfData['txt'].str.split (expand = True)\n",
    "#dfText.columns = ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "\n",
    "#for i, x in enumerate (dfText.columns):\n",
    "#    dfw.columns = [x, 'w' + str(i)]\n",
    "#    dfText = dfText.merge (dfw, how = 'left', left_on = x, right_on = x)\n",
    "\n",
    "#dfData = dfText.merge (dfData, left_index = True, right_index = True)\n",
    "\n",
    "# Convert shipment date into datetime64 format\n",
    "nastart = np.asarray([d64start + np.timedelta64 (i, 'D') for i in dfData['start']])\n",
    "\n",
    "dfData.insert (0, 'is', dfData['start'])\n",
    "dfData.drop (['start'], axis = 1, inplace = True)\n",
    "dfData.insert (0, 'start', nastart)\n",
    "\n",
    "# Create derivatives of shipment date\n",
    "dfData.insert (1, 'yy', dfData.start.dt.year)\n",
    "dfData.insert (2, 'mm', dfData.start.dt.month)\n",
    "dfData.insert (3, 'dd', dfData.start.dt.day)\n",
    "dfData.insert (4, 'wd', dfData.start.dt.dayofweek)\n",
    "\n",
    "# Create reference tables for product groups, suppliers, outbound locations, clients\n",
    "\n",
    "dfpg = pd.DataFrame (dfData['product'].unique(), columns =['product'])\n",
    "id = np.arange (1, dfpg.shape[0] + 1, dtype=int)\n",
    "rng.shuffle (id)\n",
    "dfpg['gid'] = id\n",
    "dfData = dfpg.merge (dfData, how = 'inner', left_on ='product', right_on ='product')\n",
    "filepath = Path('ref_groups.html')  \n",
    "dfpg.sort_values(['gid']).to_html (filepath, index = False)\n",
    "\n",
    "dfsp = pd.DataFrame (dfData['supplier'].unique(), columns =['supplier'])\n",
    "id = np.arange (1, dfsp.shape[0] + 1, dtype=int)\n",
    "rng.shuffle (id)\n",
    "dfsp['sid'] = id\n",
    "dfData = dfsp.merge (dfData, how = 'inner', left_on ='supplier', right_on ='supplier')\n",
    "filepath = Path('ref_suppliers.html')  \n",
    "dfsp.sort_values(['sid']).to_html (filepath, index = False)\n",
    "\n",
    "dfol = pd.DataFrame (dfData['oloc'].unique(), columns =['oloc'])\n",
    "id = np.arange (1, dfol.shape[0] + 1, dtype=int)\n",
    "rng.shuffle (id)\n",
    "dfol['oid'] = id\n",
    "dfData = dfol.merge (dfData, how = 'inner', left_on ='oloc', right_on ='oloc')\n",
    "filepath = Path('ref_olos.html')  \n",
    "dfol.sort_values(['oid']).to_html (filepath, index = False)\n",
    "\n",
    "dfcl = pd.DataFrame (dfData['client'].unique(), columns =['client'])\n",
    "id = np.arange (1, dfcl.shape[0] + 1, dtype=int)\n",
    "rng.shuffle (id)\n",
    "dfcl['cid'] = id\n",
    "dfData = dfcl.merge (dfData, how = 'inner', on ='client')\n",
    "dfData = clnt.merge (dfData, how = 'inner', on ='client')\n",
    "filepath = Path('ref_clients.html')  \n",
    "dfcl.sort_values(['cid']).to_html (filepath, index = False)\n",
    "\n",
    "fphml = Path ('inidata.html')\n",
    "lprint = [\n",
    "    'start','yy','mm','dd','client','oloc','supplier',\n",
    "    'product','nvalue','volume','packing','content',\n",
    "    'term','lt','nsl','drn','qnt','stop','phash',\n",
    "    'T00','T01','T02','T03','T04','T05'\n",
    "    ]\n",
    "dfData.loc[dfData.phash == HASHID, lprint].sort_values (by = lhash + ['start']).to_html (fphml, float_format = '{:,.2f}'.format, index=True)\n",
    "\n",
    "dfData = dfData.merge (dfdesc, how = 'inner', on = ['supplier', 'product', 'nvalue', 'volume', 'packing', 'content'])\n",
    "\n",
    "#ldrop = [\n",
    "#    'supplier', 'product', 'nvalue', 'volume', \n",
    "#    'packing', 'content', 'txtv', 'txt', 'start',\n",
    "#    'ampli', 'lt', 'term', 'client', 'oloc', 'gid',\n",
    "#    'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'T00'\n",
    "#    ]\n",
    "ldrop = [\n",
    "    'supplier', 'product', 'nvalue', 'volume', \n",
    "    'packing', 'content', 'txtv', 'start', 'ampli', \n",
    "    'lt', 'term', 'client', 'oloc', 'gid', 'T00'\n",
    "    ]\n",
    "dfData.drop (ldrop, axis=1, inplace = True)\n",
    "\n",
    "dfData = dfData.sample (frac = 1).reset_index (drop = True)\n",
    "\n",
    "# Normalize fields of _lX_ list (see below) but before save integer values \n",
    "# for 'group by' operations and testing\n",
    "\n",
    "dfData.rename (columns = {'is': 'is_i'}, inplace = True)\n",
    "dfData['tid_i'] = dfData['tid'].astype(np.int32)\n",
    "dfData['cid_i'] = dfData['cid'].astype(np.int32)\n",
    "dfData['oid_i'] = dfData['oid'].astype(np.int32)\n",
    "dfData['sid_i'] = dfData['sid'].astype(np.int32)\n",
    "dfData['did_i'] = dfData['did'].astype(np.int32)\n",
    "\n",
    "_lX_ = ['tid', 'cid', 'oid', 'sid', 'did', 'yy', 'mm', 'dd', 'wd', 'qnt', 'stop']\n",
    "\n",
    "dfX = dfData.loc(axis = 1)[_lX_]\n",
    "dfX_stats = dfX.describe().transpose()\n",
    "dfData.loc(axis = 1)[_lX_] = (dfX - dfX_stats['mean'])/dfX_stats['std']\n",
    "\n",
    "fpcsv = Path ('inidata.csv')  \n",
    "dfData.to_csv (fpcsv, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72c6c8c40433bf5d7c8161a8b67a80a03f6da0cbf5989ed4d0fcfd3194f12156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
