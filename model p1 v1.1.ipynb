{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3429/3429 [==============================] - 31s 9ms/step - loss: 7.7720 - root_mean_squared_error: 2.7878\n",
      "Epoch 2/40\n",
      "3429/3429 [==============================] - 28s 8ms/step - loss: 6.3288 - root_mean_squared_error: 2.5157\n",
      "Epoch 3/40\n",
      "3429/3429 [==============================] - 27s 8ms/step - loss: 5.9594 - root_mean_squared_error: 2.4412\n",
      "Epoch 4/40\n",
      "3429/3429 [==============================] - 27s 8ms/step - loss: 5.4081 - root_mean_squared_error: 2.3255\n",
      "Epoch 5/40\n",
      "3429/3429 [==============================] - 28s 8ms/step - loss: 5.0321 - root_mean_squared_error: 2.2432\n",
      "Epoch 6/40\n",
      "3429/3429 [==============================] - 28s 8ms/step - loss: 4.8406 - root_mean_squared_error: 2.2001\n",
      "Epoch 7/40\n",
      "3429/3429 [==============================] - 27s 8ms/step - loss: 4.5846 - root_mean_squared_error: 2.1412\n",
      "Epoch 8/40\n",
      "3429/3429 [==============================] - 27s 8ms/step - loss: 4.3411 - root_mean_squared_error: 2.0835\n",
      "Epoch 9/40\n",
      "3429/3429 [==============================] - 30s 9ms/step - loss: 4.2083 - root_mean_squared_error: 2.0514\n",
      "Epoch 10/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 4.0011 - root_mean_squared_error: 2.0003\n",
      "Epoch 11/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 3.4537 - root_mean_squared_error: 1.8584\n",
      "Epoch 12/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 2.8688 - root_mean_squared_error: 1.6938\n",
      "Epoch 13/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 2.5485 - root_mean_squared_error: 1.5964\n",
      "Epoch 14/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 2.1986 - root_mean_squared_error: 1.4828\n",
      "Epoch 15/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 1.8468 - root_mean_squared_error: 1.3590\n",
      "Epoch 16/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 1.6066 - root_mean_squared_error: 1.2675\n",
      "Epoch 17/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 1.3273 - root_mean_squared_error: 1.1521\n",
      "Epoch 18/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 1.2085 - root_mean_squared_error: 1.0993\n",
      "Epoch 19/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 1.1025 - root_mean_squared_error: 1.0500\n",
      "Epoch 20/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.9998 - root_mean_squared_error: 0.9999\n",
      "Epoch 21/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.9556 - root_mean_squared_error: 0.9775\n",
      "Epoch 22/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.8890 - root_mean_squared_error: 0.9429\n",
      "Epoch 23/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.8521 - root_mean_squared_error: 0.9231\n",
      "Epoch 24/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.8317 - root_mean_squared_error: 0.9120\n",
      "Epoch 25/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.7757 - root_mean_squared_error: 0.8807\n",
      "Epoch 26/40\n",
      "3429/3429 [==============================] - 33s 10ms/step - loss: 0.7656 - root_mean_squared_error: 0.8750\n",
      "Epoch 27/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.7376 - root_mean_squared_error: 0.8588\n",
      "Epoch 28/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.7127 - root_mean_squared_error: 0.8442\n",
      "Epoch 29/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.7051 - root_mean_squared_error: 0.8397\n",
      "Epoch 30/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.6768 - root_mean_squared_error: 0.8227\n",
      "Epoch 31/40\n",
      "3429/3429 [==============================] - 32s 9ms/step - loss: 0.6611 - root_mean_squared_error: 0.8131\n",
      "Epoch 32/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.6591 - root_mean_squared_error: 0.8119\n",
      "Epoch 33/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.6430 - root_mean_squared_error: 0.8019\n",
      "Epoch 34/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.6396 - root_mean_squared_error: 0.7997\n",
      "Epoch 35/40\n",
      "3429/3429 [==============================] - 35s 10ms/step - loss: 0.6266 - root_mean_squared_error: 0.7916\n",
      "Epoch 36/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.6083 - root_mean_squared_error: 0.7799\n",
      "Epoch 37/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.6140 - root_mean_squared_error: 0.7836\n",
      "Epoch 38/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.5975 - root_mean_squared_error: 0.7730\n",
      "Epoch 39/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.5850 - root_mean_squared_error: 0.7649\n",
      "Epoch 40/40\n",
      "3429/3429 [==============================] - 36s 10ms/step - loss: 0.5827 - root_mean_squared_error: 0.7634\n",
      "6858/6858 [==============================] - 23s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stani\\AppData\\Local\\Temp\\ipykernel_63320\\3443874638.py:147: ConvergenceWarning: Number of distinct clusters (572) found smaller than n_clusters (800). Possibly due to duplicate points in X.\n",
      "  kmeans = KMeans (n_clusters = 800).fit (naAttentionTxt)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# This script performs product identification. It builds/trains MHA + FCNN model and \n",
    "# performs clustrring of MHA output. At the begining code loads data from inidata.csv, \n",
    "# creates 2 training datasets and dataset of labels. \n",
    "# The first training set contains features such as:\n",
    "# client type, client, outbound location, supplier, year, month (1..12), day (1..31), weekday (1..7), \n",
    "# quantity (shipment's volume), stop (time threshold)\n",
    "# The second contains descriptions. Description symbols are recoded into tokens. \n",
    "# The tokenizer created in the \"character embeddings v1.0\" script is used for encoding.\n",
    "# The Embedding layer is initialized with table \"embeddings.html\" also prepared in \"character embeddings v1.0.\"\n",
    "\n",
    "# The model shown in Figure 4 (gray background) of the report is built and trained.\n",
    "# Embedded descriptions are submitted to the MHA. MHA output is combined with the first training dataset and fed \n",
    "# to deep FCNN. Labels are DoS CDF percentiles.\n",
    "# Training is performed over the entire history without a test dataset.\n",
    "\n",
    "# After training, the MHA output is fed to clustering. The clustering results are compared with \n",
    "# the correct answer. Errors are stored in the file: check_short.html\n",
    "\n",
    "WORKING_DIRECTORY = 'C:/Pilot/test/'\n",
    "os.chdir (WORKING_DIRECTORY)\n",
    "\n",
    "fpLog = Path ('log.txt')\n",
    "\n",
    "with open (fpLog, 'w') as flog:\n",
    "    print ('model p1 v1.1 starts at : ', datetime.now(), file = flog)\n",
    "\n",
    "DESC_LENGTH = 36    # DESC_LENGTH was calculated as maximum length of description\n",
    "EMBEDDING_LENGTH = 16\n",
    "\n",
    "fpini = Path ('inidata.csv')  \n",
    "#dfdata = pd.read_csv (fpini, dtype='float').fillna(0)\n",
    "dfdata = pd.read_csv (\n",
    "    fpini, \n",
    "    dtype = {\n",
    "        'tid':'float','cid':'float','oid':'float','sid':'float',\n",
    "        'yy':'float','mm':'float','dd':'float','wd':'float',\n",
    "        'is_i':np.int32,'txt':'str','nsl':'float','drn':'float','qnt':'float',\n",
    "        'stop':'float','phash':np.int64,'T01':'float','T02':'float','T03':'float',\n",
    "        'T04':'float','T05':'float','T06':'float','T07':'float','T08':'float',\n",
    "        'T09':'float','T10':'float','T11':'float','T12':'float','T13':'float',\n",
    "        'T14':'float','T15':'float','T16':'float','T17':'float','T18':'float',\n",
    "        'T19':'float','T20':'float','did':'float',\n",
    "        'tid_i':np.int32,'cid_i':np.int32,'oid_i':np.int32,'sid_i':np.int32,'did_i':np.int32\n",
    "    }\n",
    "    ).fillna(0)\n",
    "\n",
    "dfdata = dfdata.sample (frac=1).reset_index (drop = True)\n",
    "\n",
    "#fpwid = Path ('ref_words.html')  \n",
    "#dfw = pd.read_html (fpwid, encoding = 'UTF-8')[0]\n",
    "#dfw.set_index ('wid', inplace = True)\n",
    "#dfw.loc[0] = ['']\n",
    "\n",
    "#dfdata['txt'] = [\n",
    "#    ' '.join ((dfw.word[w0],dfw.word[w1],dfw.word[w2],dfw.word[w3],dfw.word[w4],dfw.word[w5],dfw.word[w6])).ljust (DESC_LENGTH, ' ')\n",
    "#    for w0, w1, w2, w3, w4, w5, w6 in zip (dfdata.w0,dfdata.w1,dfdata.w2,dfdata.w3,dfdata.w4,dfdata.w5,dfdata.w6)\n",
    "#    ]\n",
    "\n",
    "#dfdata.drop (['w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6'], axis = 1, inplace = True)\n",
    "\n",
    "_lX_ = ['tid', 'cid', 'oid', 'sid', 'yy', 'mm', 'dd', 'wd', 'qnt', 'stop']\n",
    "_lY_ = ['T01','T02','T03','T04','T05','T06','T07','T08','T09','T10', \n",
    "        'T11','T12','T13','T14','T15','T16','T17','T18','T19','T20']\n",
    "\n",
    "#dfX = dfdata.loc(axis = 1)[_lX_]\n",
    "#dfX_stats = dfX.describe().transpose()\n",
    "#dfdata.loc(axis = 1)[_lX_] = (dfX - dfX_stats['mean'])/dfX_stats['std']\n",
    "\n",
    "#dstrain = dfdata\n",
    "\n",
    "fpT = Path ('tokenizer.pickle')  \n",
    "with open (fpT, 'rb') as handle:\n",
    "    tokenizer = pickle.load (handle)\n",
    "\n",
    "#naTraintxt = np.array (tokenizer.texts_to_sequences (dstrain.txt), dtype = np.int32) \n",
    "#naYtrain = np.array (dstrain.loc(axis = 1)[_lY_])\n",
    "#naXtrain = np.array (dstrain.loc(axis = 1)[_lX_])\n",
    "\n",
    "#ltext = [\" \".join(x.split()) for x in df.txt.values.tolist()]\n",
    "ltxt = [\" \".join(x.split()).ljust (DESC_LENGTH) for x in dfdata.txt.values.tolist()]\n",
    "naTraintxt = np.array (tokenizer.texts_to_sequences (ltxt), dtype = np.int32) \n",
    "naYtrain   = np.array (dfdata.loc(axis = 1)[_lY_])\n",
    "naXtrain   = np.array (dfdata.loc(axis = 1)[_lX_])\n",
    "\n",
    "trInputShipment = layers.Input (shape = (naXtrain.shape[1], ), name = \"shipment\")\n",
    "trInputDesc = layers.Input (shape = (DESC_LENGTH, ), name = \"desc\") \n",
    "\n",
    "# Loading embedding matrix\n",
    "\n",
    "fpE = Path ('embeddings.html')  \n",
    "dfe = pd.read_html (fpE, encoding = 'UTF-8')[0].sort_values (['eid'], ignore_index = True)\n",
    "dfe.drop (['char', 'tkid', 'eid'], axis=1, inplace = True)\n",
    "\n",
    "naEmbeddingMatrix = np.array (dfe)\n",
    "\n",
    "# Embed each character in the description into a 16-dimensional vector\n",
    "trEmbedding = layers.Embedding (\n",
    "    len (tokenizer.word_index) + 1,\n",
    "    EMBEDDING_LENGTH,\n",
    "    embeddings_initializer = keras.initializers.Constant (naEmbeddingMatrix),\n",
    "    trainable = False,\n",
    "    mask_zero = True\n",
    ")(trInputDesc)\n",
    "\n",
    "ktrMHA = layers.MultiHeadAttention (num_heads = 4, key_dim = 4)(trEmbedding, trEmbedding)\n",
    "\n",
    "trMHAreduced = layers.Lambda (lambda xin: tf.keras.backend.sum (xin, axis=-2), name = 'LAYER_LAMBDA') (ktrMHA)\n",
    "\n",
    "ktr = layers.concatenate ([trInputShipment, trMHAreduced], name = 'LAYER_CONCATENATE')\n",
    "\n",
    "ktr = layers.Dense (units ='256', activation ='elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "ktr = layers.Dense (units ='256', activation ='elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "ktr = layers.Dense (units ='256', activation ='elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "ktr = layers.Dense (units ='256', activation ='elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "lrout = layers.Dense (units ='20', activation ='relu', name ='output')(ktr)\n",
    "\n",
    "mdSimple = keras.Model (inputs = [trInputDesc, trInputShipment], outputs = lrout)\n",
    "\n",
    "#mdSimple.summary()\n",
    "\n",
    "iBatchSize = 64\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam (learning_rate = 0.001)#(lr_schedule)\n",
    "\n",
    "mdSimple.compile (\n",
    "    optimizer = optimizer, \n",
    "    loss = tf.keras.losses.MeanSquaredError(), \n",
    "    metrics = tf.keras.metrics.RootMeanSquaredError()\n",
    "    )\n",
    "\n",
    "history = mdSimple.fit (\n",
    "    {\"shipment\": naXtrain, \"desc\": naTraintxt},\n",
    "    naYtrain,\n",
    "    epochs = 100, \n",
    "    batch_size = iBatchSize \n",
    "    )\n",
    "\n",
    "#naTxt = np.array (tokenizer.texts_to_sequences (dfdata.txt), dtype = np.int32) \n",
    "\n",
    "mdSub = models.Model (mdSimple.inputs, mdSimple.get_layer ('LAYER_LAMBDA').output)   \n",
    "#naAttentionTxt = mdSub.predict ({\"shipment\": dfdata.loc[:,_lX_].to_numpy(), \"desc\": naTxt})\n",
    "naAttentionTxt = mdSub.predict ({\"shipment\": dfdata.loc[:,_lX_].to_numpy(), \"desc\": naTraintxt})\n",
    "\n",
    "kmeans = KMeans (n_clusters = 800).fit (naAttentionTxt)\n",
    "#print ('kmeans.n_iter_: ', kmeans.n_iter_)\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('kmeans.n_iter_: ', kmeans.n_iter_, file = flog)\n",
    "\n",
    "naCluster_id = kmeans.predict (naAttentionTxt)\n",
    "dfdata['cluster_id'] = naCluster_id\n",
    "\n",
    "iUniq = len (pd.unique (dfdata['cluster_id']))\n",
    "#print ('unique values in cluster_id: ', iUniq)\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('unique values in cluster_id: ', iUniq, file = flog)\n",
    "\n",
    "df = dfdata.groupby('cluster_id')['txt'].agg(['unique'])\n",
    "df['cn'] = [len(x) for x in df.unique]\n",
    "\n",
    "fplong = Path ('check_long.html')\n",
    "df.loc (axis = 1)[['cn', 'unique']].to_html (fplong, index = True)\n",
    "fpshort = Path ('check_short.html')\n",
    "fpshortad = Path ('check_short_add.html')\n",
    "dferr = df.loc (axis = 0)[df.cn != 3]\n",
    "dferr.to_html (fpshort, index = True)\n",
    "dferr.reset_index (inplace = True)\n",
    "dferr = dferr.rename (columns = {'index':'cluster_id'})\n",
    "dferr = dferr.merge (dfdata.loc (axis = 1)[['tid', 'cid', 'oid', 'sid', 'cluster_id', 'yy', 'mm', 'qnt']], how = 'inner', on = ('cluster_id'))\n",
    "dferr = dferr.groupby(['tid', 'cid', 'oid', 'sid', 'cluster_id', 'yy', 'mm', 'qnt'])[['tid']].count()\n",
    "dferr.to_html (fpshortad, index = True)\n",
    "\n",
    "#dfdata.drop (['txt'], axis = 1, inplace = True)\n",
    "\n",
    "#fpcsv = Path ('intermediate.csv')  \n",
    "#dfdata.to_csv (fpcsv, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72c6c8c40433bf5d7c8161a8b67a80a03f6da0cbf5989ed4d0fcfd3194f12156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
