{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " INPUT_A (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               2816      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 20)                5140      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209,428\n",
      "Trainable params: 207,380\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2845/2845 [==============================] - 16s 5ms/step - loss: 8.1647 - root_mean_squared_error: 2.8384 - val_loss: 6.7940 - val_root_mean_squared_error: 2.5815\n",
      "Epoch 2/10\n",
      "2845/2845 [==============================] - 19s 7ms/step - loss: 6.7640 - root_mean_squared_error: 2.5720 - val_loss: 6.3612 - val_root_mean_squared_error: 2.4892\n",
      "Epoch 3/10\n",
      "2845/2845 [==============================] - 19s 7ms/step - loss: 6.4393 - root_mean_squared_error: 2.5021 - val_loss: 6.0862 - val_root_mean_squared_error: 2.4279\n",
      "Epoch 4/10\n",
      "2845/2845 [==============================] - 19s 7ms/step - loss: 6.2813 - root_mean_squared_error: 2.4653 - val_loss: 6.2736 - val_root_mean_squared_error: 2.4616\n",
      "Epoch 5/10\n",
      "2845/2845 [==============================] - 17s 6ms/step - loss: 6.0771 - root_mean_squared_error: 2.4196 - val_loss: 5.8654 - val_root_mean_squared_error: 2.3743\n",
      "Epoch 6/10\n",
      "2845/2845 [==============================] - 18s 6ms/step - loss: 5.6645 - root_mean_squared_error: 2.3300 - val_loss: 5.1821 - val_root_mean_squared_error: 2.2226\n",
      "Epoch 7/10\n",
      "2845/2845 [==============================] - 14s 5ms/step - loss: 5.4187 - root_mean_squared_error: 2.2741 - val_loss: 5.2707 - val_root_mean_squared_error: 2.2402\n",
      "Epoch 8/10\n",
      "2845/2845 [==============================] - 14s 5ms/step - loss: 5.1999 - root_mean_squared_error: 2.2237 - val_loss: 4.8373 - val_root_mean_squared_error: 2.1400\n",
      "Epoch 9/10\n",
      "2845/2845 [==============================] - 14s 5ms/step - loss: 5.0862 - root_mean_squared_error: 2.1964 - val_loss: 5.8484 - val_root_mean_squared_error: 2.3630\n",
      "Epoch 10/10\n",
      "2845/2845 [==============================] - 14s 5ms/step - loss: 4.8534 - root_mean_squared_error: 2.1415 - val_loss: 4.1157 - val_root_mean_squared_error: 1.9606\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Script\n",
    "# - loads data from inidata.csv\n",
    "# - generates training and test data sets for the predictive FCNN model.\n",
    "# - Creates and trains a predictive model.  The learning history is written to the file 'history_p3.txt'\n",
    "\n",
    "# Basic actions:\n",
    "# 1. The test data set is formed from the last 360 days of history\n",
    "# 2. Script selects the following features for the predictive model: client type, client, outbound location, supplier,\n",
    "# description group id, consisting of 3 descriptions that match in meaning but have different word orders,\n",
    "# month (1..12), day(1..31), weekday(1..7), quantity (shipment volume), stop (time threshold)\n",
    "\n",
    "WORKING_DIRECTORY = 'C:/Pilot/test/'\n",
    "os.chdir (WORKING_DIRECTORY)\n",
    "\n",
    "fpLog = Path ('log.txt')\n",
    "\n",
    "with open (fpLog, 'w') as flog:\n",
    "    print ('model p3 v1.3 starts at : ', datetime.now(), file = flog)\n",
    "\n",
    "#fpini = Path ('intermediate.csv')  \n",
    "fpini = Path ('inidata.csv')\n",
    "dfdata = pd.read_csv (\n",
    "    fpini, \n",
    "    dtype = {\n",
    "        'tid':'float','cid':'float','oid':'float','sid':'float',\n",
    "        'yy':'float','mm':'float','dd':'float','wd':'float',\n",
    "        'is_i':np.int32,'txt':'str','nsl':'float','drn':'float','qnt':'float',\n",
    "        'stop':'float','phash':np.int64,'T01':'float','T02':'float','T03':'float',\n",
    "        'T04':'float','T05':'float','T06':'float','T07':'float','T08':'float',\n",
    "        'T09':'float','T10':'float','T11':'float','T12':'float','T13':'float',\n",
    "        'T14':'float','T15':'float','T16':'float','T17':'float','T18':'float',\n",
    "        'T19':'float','T20':'float','did':'float',\n",
    "        'tid_i':np.int32,'cid_i':np.int32,'oid_i':np.int32,'sid_i':np.int32,'did_i':np.int32\n",
    "    }\n",
    "    ).fillna(0)\n",
    "\n",
    "#dfdata = pd.read_csv (fpini, dtype = 'float').fillna(0)\n",
    "#dfdata.rename (columns = {'is': 'is_i'}, inplace = True)\n",
    "#dfdata['tid_i'] = dfdata['tid'].astype(np.int32)\n",
    "#dfdata['cid_i'] = dfdata['cid'].astype(np.int32)\n",
    "#dfdata['oid_i'] = dfdata['oid'].astype(np.int32)\n",
    "#dfdata['sid_i'] = dfdata['sid'].astype(np.int32)\n",
    "\n",
    "#dfdata['did_i'] = dfdata['did'].astype(np.int32)\n",
    "#dfdid_stats = dfdata['did'].describe().transpose()\n",
    "\n",
    "#dfdata['did'] = (dfdata['did'] - dfdid_stats['mean'])/dfdid_stats['std']\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "ismax = np.max (dfdata.loc(axis = 1)['is_i'])\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('ismax: ', ismax, file = flog)\n",
    "\n",
    "dftest  = dfdata.loc[dfdata.is_i >= (ismax - 360)].copy()\n",
    "dftrain = dfdata.loc[dfdata.is_i <  (ismax - 360)].copy()\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('dftest: ',  dftest.shape,  file = flog)\n",
    "    print ('dftrain: ', dftrain.shape, file = flog)\n",
    "\n",
    "lx     = ['tid', 'cid', 'oid', 'sid', 'did', 'mm', 'dd', 'wd', 'qnt', 'stop']\n",
    "ly    = [\"T%02d\" % (i,) for i in range(1, 21)]\n",
    "    \n",
    "dfXtrain, dfYtrain = dftrain.loc(axis = 1)[lx], dftrain.loc(axis = 1)[ly]\n",
    "dfXtest,  dfYtest  = dftest.loc (axis = 1)[lx], dftest.loc (axis = 1)[ly]\n",
    "\n",
    "ktrInputA = layers.Input (shape = (dfXtrain.shape[1], ), name = \"INPUT_A\")\n",
    "\n",
    "ktr = layers.Dense (units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktrInputA)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktr = layers.Dense (units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktr = layers.Dense (units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktr = layers.Dense (units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktrOut = layers.Dense (units ='20', kernel_regularizer = regularizers.l2(0.0001), activation = 'relu')(ktr)\n",
    "\n",
    "mdSimple_h = keras.Model (inputs = [ktrInputA], outputs = ktrOut)\n",
    "\n",
    "mdSimple_h.summary()\n",
    "\n",
    "iBatchSize = 64\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam (learning_rate = 0.001)\n",
    "\n",
    "mdSimple_h.compile (\n",
    "    optimizer = optimizer, \n",
    "    loss = tf.keras.losses.MeanSquaredError(), \n",
    "    metrics = tf.keras.metrics.RootMeanSquaredError()\n",
    "    )\n",
    "\n",
    "history_h = mdSimple_h.fit (\n",
    "    {\"INPUT_A\": dfXtrain.to_numpy()},\n",
    "    dfYtrain.to_numpy(),\n",
    "    epochs = 100,\n",
    "    batch_size = iBatchSize, \n",
    "    validation_data = ({\"INPUT_A\": dfXtest.to_numpy()}, dfYtest.to_numpy())\n",
    "    )\n",
    "\n",
    "fphist = Path ('history_p3.txt')\n",
    "dfhist = pd.DataFrame (history_h.history)\n",
    "dfhist.to_csv (fphist, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72c6c8c40433bf5d7c8161a8b67a80a03f6da0cbf5989ed4d0fcfd3194f12156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
