{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnPrepareData starts to prepare train data\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " INPUT_H (InputLayer)           [(None, 32, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 32, 32)       0           ['INPUT_H[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 128)          82432       ['masking[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           2064        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " INPUT_A (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 16)          64          ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " LAYER_CONCATENATE (Concatenate  (None, 27)          0           ['INPUT_A[0][0]',                \n",
      " )                                                                'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          7168        ['LAYER_CONCATENATE[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          65792       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 256)         1024        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          65792       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          65792       ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 20)           5140        ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 298,340\n",
      "Trainable params: 296,260\n",
      "Non-trainable params: 2,080\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "2795/2795 [==============================] - 140s 49ms/step - loss: 5.1173 - root_mean_squared_error: 2.2396 - val_loss: 2.9476 - val_root_mean_squared_error: 1.6834\n",
      "Epoch 2/10\n",
      "2795/2795 [==============================] - 133s 48ms/step - loss: 2.6114 - root_mean_squared_error: 1.5777 - val_loss: 2.3278 - val_root_mean_squared_error: 1.4828\n",
      "Epoch 3/10\n",
      "2795/2795 [==============================] - 134s 48ms/step - loss: 2.0448 - root_mean_squared_error: 1.3812 - val_loss: 2.0016 - val_root_mean_squared_error: 1.3623\n",
      "Epoch 4/10\n",
      "2795/2795 [==============================] - 134s 48ms/step - loss: 1.6607 - root_mean_squared_error: 1.2285 - val_loss: 1.4371 - val_root_mean_squared_error: 1.1323\n",
      "Epoch 5/10\n",
      "2795/2795 [==============================] - 135s 48ms/step - loss: 1.4891 - root_mean_squared_error: 1.1525 - val_loss: 1.3818 - val_root_mean_squared_error: 1.1032\n",
      "Epoch 6/10\n",
      "2795/2795 [==============================] - 136s 49ms/step - loss: 1.3726 - root_mean_squared_error: 1.0970 - val_loss: 1.6731 - val_root_mean_squared_error: 1.2256\n",
      "Epoch 7/10\n",
      "2795/2795 [==============================] - 135s 48ms/step - loss: 1.2791 - root_mean_squared_error: 1.0516 - val_loss: 1.3384 - val_root_mean_squared_error: 1.0788\n",
      "Epoch 8/10\n",
      "2795/2795 [==============================] - 137s 49ms/step - loss: 1.1889 - root_mean_squared_error: 1.0068 - val_loss: 1.2780 - val_root_mean_squared_error: 1.0489\n",
      "Epoch 9/10\n",
      "2795/2795 [==============================] - 136s 48ms/step - loss: 1.1761 - root_mean_squared_error: 0.9994 - val_loss: 1.0318 - val_root_mean_squared_error: 0.9239\n",
      "Epoch 10/10\n",
      "2795/2795 [==============================] - 136s 49ms/step - loss: 1.1125 - root_mean_squared_error: 0.9667 - val_loss: 1.2353 - val_root_mean_squared_error: 1.0284\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Script\n",
    "# - loads data from inidata.csv\n",
    "# - generates training and test data sets for the FCNN + RNN predictive model.\n",
    "# - Creates and trains a predictive model. Writes the learning history to the file 'history_p2.txt'\n",
    "\n",
    "# Basic actions:\n",
    "# 1. The test set is formed from the last 360 days of history simulated\n",
    "# 2. To build time series of shipments, the script sorts history of shipments by fields: \n",
    "# customer type, customer, outbound location, supplier, \n",
    "# description group id, consisting of 3 descriptions that match in meaning but have different word orders, \n",
    "# shipment date. The script then groups shipments by the same fields, but without the shipment date.  \n",
    "# Groups are placed in lists and combined with the history by the grouping fields.\n",
    "# Those, in each row of the history (shipment) the new field appears with a list of all shimpents of the same group.  \n",
    "# Each list is shortened to HISTORY_LENGTH of shipments that arrived at the warehouse before \n",
    "# the shipment/row to which the list belongs. Shipment-\"owner\" is not included in the final series. Rows in series contain \n",
    "# enhanced set of fields. Besids mentioned above fields RNN rows include: duration of shipmemt, not saled quantity, labels. \n",
    "# Since series consists of shipments till forecasting, we can use all shipments features. \n",
    "# A tensor with the time series in a separate dimension is formed.\n",
    "\n",
    "WORKING_DIRECTORY = 'C:/Pilot/test/'\n",
    "os.chdir (WORKING_DIRECTORY)\n",
    "\n",
    "fpLog = Path ('log.txt')\n",
    "\n",
    "with open (fpLog, 'w') as flog:\n",
    "    print ('model p2 v1.3 starts at : ', datetime.now(), file = flog)\n",
    "\n",
    "#fpini = Path ('intermediate.csv')  \n",
    "fpini = Path ('inidata.csv')\n",
    "dfdata = pd.read_csv (\n",
    "    fpini, \n",
    "    dtype = {\n",
    "        'tid':'float','cid':'float','oid':'float','sid':'float',\n",
    "        'yy':'float','mm':'float','dd':'float','wd':'float',\n",
    "        'is_i':np.int32,'txt':'str','nsl':'float','drn':'float','qnt':'float',\n",
    "        'stop':'float','phash':np.int64,'T01':'float','T02':'float','T03':'float',\n",
    "        'T04':'float','T05':'float','T06':'float','T07':'float','T08':'float',\n",
    "        'T09':'float','T10':'float','T11':'float','T12':'float','T13':'float',\n",
    "        'T14':'float','T15':'float','T16':'float','T17':'float','T18':'float',\n",
    "        'T19':'float','T20':'float','did':'float',\n",
    "        'tid_i':np.int32,'cid_i':np.int32,'oid_i':np.int32,'sid_i':np.int32,'did_i':np.int32\n",
    "    }\n",
    "    ).fillna(0)\n",
    "\n",
    "#dfdata = pd.read_csv (fpini, dtype = 'float').fillna(0)\n",
    "\n",
    "#dfdata['did_i'] = dfdata['did'].astype(np.int32)\n",
    "#dfdid_stats = dfdata['did'].describe().transpose()\n",
    "\n",
    "#dfdata['did'] = (dfdata['did'] - dfdid_stats['mean'])/dfdid_stats['std']\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "ismax = np.max (dfdata.loc(axis = 1)['is_i'])\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('ismax: ', ismax, file = flog)\n",
    "\n",
    "dftest  = dfdata.loc[dfdata.is_i >= (ismax - 360)].copy()\n",
    "dftrain = dfdata.loc[dfdata.is_i <  (ismax - 360)].copy()\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('dftest: ',  dftest.shape,  file = flog)\n",
    "    print ('dftrain: ', dftrain.shape, file = flog)\n",
    "\n",
    "lsort = ['tid_i', 'cid_i', 'oid_i', 'sid_i', 'did_i', 'is_i']\n",
    "lgb   = ['tid_i', 'cid_i', 'oid_i', 'sid_i', 'did_i']\n",
    "lx1   = ['tid', 'cid', 'oid', 'sid', 'did', 'mm', 'dd', 'wd', 'qnt', 'stop']\n",
    "lx2   = ['drn', 'nsl']\n",
    "ly    = [\"T%02d\" % (i,) for i in range(1, 21)]\n",
    "lxd   = lx1\n",
    "lxa   = lx1 + lx2 + ly\n",
    "\n",
    "# Full list of columns: lsort + lxd  + lxa + ly\n",
    "\n",
    "HISTORY_LENGTH = 32\n",
    "ROW_LENGTH = len (lxa)\n",
    "\n",
    "def fnPrepareData (df):\n",
    "    \n",
    "    df.sort_values (by = lsort, inplace = True)\n",
    "\n",
    "    dfgb = df.groupby(lgb)\n",
    "    df['gn'] = dfgb.cumcount()\n",
    "    dfgb = df.loc (axis = 1)[lxa + lgb].groupby (lgb)\n",
    "    dflist = dfgb.agg (list)\n",
    "\n",
    "    dfocean = df.merge (dflist, how = 'inner', on = lgb, suffixes = [\"\", \"_l\"])\n",
    "\n",
    "    dfocean.drop (dfocean[dfocean.gn <= 2].index, inplace = True)\n",
    "\n",
    "    dfocean['lol'] = dfocean.loc (axis = 1)[[x + '_l' for x in lxa]].values.tolist()\n",
    "\n",
    "    dfocean = dfocean.sample (frac = 1).reset_index (drop = True)\n",
    "\n",
    "    lol = [[y[max (ind - HISTORY_LENGTH, 0): ind] for y in x] for x, ind in zip (dfocean['lol'], dfocean['gn'])]\n",
    "\n",
    "    trA = tf.ragged.constant (lol).to_tensor()\n",
    "    trA = tf.transpose (trA, perm = [0, 2, 1])\n",
    "    \n",
    "    dfX = dfocean.loc(axis = 1)[lxd + ['gn']].copy (deep = True)\n",
    "    dfY = dfocean.loc(axis = 1)[ ly].copy (deep = True)\n",
    "\n",
    "    return trA, dfX, dfY\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('fnPrepareData starts to prepare test data', file = flog)\n",
    "\n",
    "trAtest,  dfXtest,  dfYtest  = fnPrepareData (dftest)\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('Test data is ready', file = flog)\n",
    "    print ('fnPrepareData starts to prepare train data', file = flog)\n",
    "\n",
    "trAtrain, dfXtrain, dfYtrain = fnPrepareData (dftrain)\n",
    "\n",
    "with open (fpLog, 'a') as flog:\n",
    "    print ('Train data is ready', file = flog)\n",
    "    print ('trAtrain: ', trAtrain.shape, file = flog)\n",
    "    print ('dfXtrain: ', dfXtrain.shape, file = flog)\n",
    "    print ('dfYtrain: ', dfYtrain.shape, file = flog)\n",
    "\n",
    "ktrInputA = layers.Input (shape = (dfXtrain.shape[1], ), name = \"INPUT_A\")\n",
    "\n",
    "ktrInputH = layers.Input (shape = (HISTORY_LENGTH, ROW_LENGTH), name = \"INPUT_H\") \n",
    "\n",
    "ktrMask = layers.Masking (mask_value = 0.)(ktrInputH)\n",
    "\n",
    "ktrRNN = layers.LSTM (128)(ktrMask)\n",
    "ktr = layers.Dense (units ='16', activation ='elu')(ktrRNN)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "\n",
    "ktr = layers.concatenate ([ktrInputA, ktr], name = 'LAYER_CONCATENATE')\n",
    "#ktr = layers.Dense (units ='256', activation ='elu')(ktr)\n",
    "#ktr = layers.Dropout(0.1)(ktr)\n",
    "ktr = layers.Dense(units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.1)(ktr)\n",
    "ktr = layers.Dense(units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktr = layers.Dense(units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktr = layers.Dense(units ='256', kernel_regularizer = regularizers.l2(0.0001), activation = 'elu')(ktr)\n",
    "ktr = layers.BatchNormalization ()(ktr)\n",
    "#ktr = layers.Dropout(0.05)(ktr)\n",
    "ktrOut = layers.Dense(units ='20', kernel_regularizer = regularizers.l2(0.0001), activation = 'relu')(ktr)\n",
    "\n",
    "mdSimple_h = keras.Model (inputs = [ktrInputA, ktrInputH], outputs = ktrOut)\n",
    "\n",
    "mdSimple_h.summary()\n",
    "\n",
    "iBatchSize = 64\n",
    "\n",
    "#optimizer = tf.keras.optimizers.SGD (learning_rate = 0.001)#(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam (learning_rate = 0.001)\n",
    "\n",
    "mdSimple_h.compile (\n",
    "    optimizer = optimizer, \n",
    "    loss = tf.keras.losses.MeanSquaredError(), \n",
    "#    loss = tf.keras.losses.MeanSquaredLogarithmicError(), \n",
    "    metrics = tf.keras.metrics.RootMeanSquaredError()\n",
    "    )\n",
    "\n",
    "history_h = mdSimple_h.fit (\n",
    "    {\"INPUT_A\": dfXtrain.to_numpy(), \"INPUT_H\": trAtrain},\n",
    "    dfYtrain.to_numpy(),\n",
    "    epochs = 100,\n",
    "    batch_size = iBatchSize, \n",
    "    validation_data = ({\"INPUT_A\": dfXtest.to_numpy(), \"INPUT_H\": trAtest}, dfYtest.to_numpy())\n",
    "    )\n",
    "\n",
    "fphist = Path ('history_p2.txt')\n",
    "dfhist = pd.DataFrame (history_h.history)\n",
    "dfhist.to_csv (fphist, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72c6c8c40433bf5d7c8161a8b67a80a03f6da0cbf5989ed4d0fcfd3194f12156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
